# BÃO CÃO Äá»’ ÃN MÃ”N Há»ŒC SE360  
## Module B: Thiáº¿t káº¿ cho Reliability & High Availability

**NhÃ³m:** UIT-Go Team  
**ThÃ nh viÃªn:**  
- VÃµ Minh Kiá»‡t â€“ 22520727  
- VÃµ Mai NguyÃªn â€“ 22520991  

**Module ChuyÃªn sÃ¢u:** Module B â€“ Thiáº¿t káº¿ cho Reliability & High Availability  
**Vai trÃ²:** Site Reliability Engineer (SRE)

---

## 1. Tá»•ng quan & Bá»‘i cáº£nh BÃ i toÃ¡n (Business Context)

### 1.1. Bá»‘i cáº£nh
UIT-Go Ä‘ang trong giai Ä‘oáº¡n "Scale or Die" vá»›i sá»± tÄƒng trÆ°á»Ÿng ngÆ°á»i dÃ¹ng Ä‘á»™t biáº¿n.  
Má»—i phÃºt downtime cÃ³ thá»ƒ gÃ¢y **máº¥t mÃ¡t $5,000 doanh thu** vÃ  áº£nh hÆ°á»Ÿng nghiÃªm trá»ng Ä‘áº¿n uy tÃ­n thÆ°Æ¡ng hiá»‡u.

### 1.2. ThÃ¡ch thá»©c Reliability & HA
Há»‡ thá»‘ng cÅ© gáº·p cÃ¡c váº¥n Ä‘á» nghiÃªm trá»ng vá» Ä‘á»™ tin cáº­y:

#### **Single Point of Failure (SPOF)**
- Database cháº¡y trÃªn single AZ â†’ nguy cÆ¡ máº¥t dá»¯ liá»‡u cao  
- Application server Ä‘Æ¡n láº» â†’ khÃ´ng chá»‹u Ä‘Æ°á»£c táº£i cao vÃ  sá»± cá»‘

#### **Thá»i gian phá»¥c há»“i dÃ i**
- KhÃ´ng cÃ³ káº¿ hoáº¡ch Disaster Recovery â†’ RTO cÃ³ thá»ƒ lÃªn Ä‘áº¿n 4-6 giá»  
- Backup thá»§ cÃ´ng â†’ RPO khÃ´ng Ä‘Æ°á»£c Ä‘áº£m báº£o

### 1.3. Má»¥c tiÃªu Module B
Vá»›i vai trÃ² Site Reliability Engineer:

- **Availability**: Äáº¡t 99.9% uptime (â‰¤ 8.77 giá» downtime/nÄƒm)  
- **Recovery**: RTO â‰¤ 30 phÃºt, RPO â‰¤ 5 phÃºt  
- **Fault Tolerance**: Há»‡ thá»‘ng tá»± phá»¥c há»“i khi gáº·p sá»± cá»‘

---

## 2. Kiáº¿n trÃºc High Availability Multi-AZ

### 2.1. SÆ¡ Ä‘á»“ Kiáº¿n trÃºc Triá»ƒn khai

```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚            INTERNET                 â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚       Internet Gateway (IGW)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        VPC (10.10.0.0/16)                        â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚    AZ: ap-southeast-1a  â”‚  â”‚    AZ: ap-southeast-1b  â”‚       â”‚
â”‚  â”‚                         â”‚  â”‚                         â”‚       â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚       â”‚
â”‚  â”‚  â”‚Public Subnet        â”‚â”‚  â”‚â”‚Public Subnet        â”‚  â”‚       â”‚
â”‚  â”‚  â”‚10.10.1.0/24         â”‚â”‚  â”‚â”‚10.10.2.0/24         â”‚  â”‚       â”‚
â”‚  â”‚  â”‚                     â”‚â”‚  â”‚â”‚                     â”‚  â”‚       â”‚
â”‚  â”‚  â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚  â”‚â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚       â”‚
â”‚  â”‚  â”‚ â”‚   EC2 Instance  â”‚ â”‚â”‚  â”‚â”‚ â”‚   EC2 Instance  â”‚ â”‚  â”‚       â”‚
â”‚  â”‚  â”‚ â”‚                 â”‚ â”‚â”‚  â”‚â”‚ â”‚                 â”‚ â”‚  â”‚       â”‚
â”‚  â”‚  â”‚ â”‚ â€¢ Microservices â”‚ â”‚â”‚  â”‚â”‚ â”‚ â€¢ Microservices â”‚ â”‚  â”‚       â”‚
â”‚  â”‚  â”‚ â”‚ â€¢ MongoDB       â”‚ â”‚â”‚  â”‚â”‚ â”‚ â€¢ MongoDB       â”‚ â”‚  â”‚       â”‚
â”‚  â”‚  â”‚ â”‚ â€¢ Kafka         â”‚ â”‚â”‚  â”‚â”‚ â”‚ â€¢ Kafka         â”‚ â”‚  â”‚       â”‚
â”‚  â”‚  â”‚ â”‚ â€¢ Redis         â”‚ â”‚â”‚  â”‚â”‚ â”‚ â€¢ Redis         â”‚ â”‚  â”‚       â”‚
â”‚  â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚  â”‚â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚       â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚  â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                   â”‚                         â”‚                   â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                 â”‚                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚              Application Load Balancer (ALB)             â”‚ â”‚
â”‚  â”‚                                                          â”‚ â”‚
â”‚  â”‚  â€¢ Health Check: /health                                 â”‚ â”‚
â”‚  â”‚  â€¢ Cross-AZ Load Distribution                            â”‚ â”‚
â”‚  â”‚  â€¢ Auto-failover khi node down                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2. ThÃ nh pháº§n Kiáº¿n trÃºc HA

| ThÃ nh pháº§n | Cáº¥u hÃ¬nh HA | Má»¥c Ä‘Ã­ch |
|------------|-------------|----------|
| **Application Load Balancer** | Cross-AZ distribution | PhÃ¢n phá»‘i táº£i, auto-failover |
| **EC2 Instances** | 2 instances across AZs | Redundancy, zero downtime deployment |
| **MongoDB Cluster** | Replica Set 3 nodes | Data replication, automatic failover |
| **Kafka Cluster** | Multi-broker setup | Message queue reliability |
| **Redis** | Master-Slave replication | Session/cache availability |

---

## 3. CÃ¡c Quyáº¿t Ä‘á»‹nh Thiáº¿t káº¿ & PhÃ¢n tÃ­ch Trade-off

### 3.1. Quyáº¿t Ä‘á»‹nh 1: Multi-AZ Deployment

#### **Váº¥n Ä‘á»**
- Single AZ deployment táº¡o SPOF nghiÃªm trá»ng  
- Khi AZ gáº·p sá»± cá»‘, toÃ n bá»™ há»‡ thá»‘ng ngá»«ng hoáº¡t Ä‘á»™ng

#### **Giáº£i phÃ¡p**
Triá»ƒn khai **Multi-AZ vá»›i ALB** Ä‘á»ƒ phÃ¢n phá»‘i táº£i:
- 2 EC2 instances trÃªn 2 AZ khÃ¡c nhau  
- ALB health check tá»± Ä‘á»™ng loáº¡i bá» node khÃ´ng khá»e máº¡nh

#### **Trade-off**
| Loáº¡i | PhÃ¢n tÃ­ch |
|------|-----------|
| **Gain â€“ Availability** | 99.9% uptime, tá»± Ä‘á»™ng failover trong vÃ i giÃ¢y |
| **Loss â€“ Cost** | Chi phÃ­ tÄƒng gáº¥p Ä‘Ã´i cho infrastructure |
| **Loss â€“ Complexity** | Phá»©c táº¡p hÆ¡n trong deployment vÃ  monitoring |

**Quyáº¿t Ä‘á»‹nh cháº¥p nháº­n:**  
> Vá»›i business impact $5,000/phÃºt, chi phÃ­ tÄƒng gáº¥p Ä‘Ã´i lÃ  hoÃ n toÃ n há»£p lÃ½.

---

### 3.2. Quyáº¿t Ä‘á»‹nh 2: Database Replication Strategy

#### **Váº¥n Ä‘á»**
- Single database instance â†’ máº¥t dá»¯ liá»‡u khi sá»± cá»‘  
- Backup/restore thá»§ cÃ´ng â†’ RTO/RPO khÃ´ng Ä‘áº£m báº£o

#### **Giáº£i phÃ¡p**
Sá»­ dá»¥ng **MongoDB Replica Set**:
- Primary node (write), 2 Secondary nodes (read)  
- Automatic failover trong 10-30 giÃ¢y

#### **Trade-off**
| ÄÆ°á»£c | Máº¥t |
|------|------|
| RPO gáº§n nhÆ° báº±ng 0 | TÄƒng 3x storage cost |
| RTO < 30 giÃ¢y cho database | Read performance phÃ¢n tÃ¡n |
| Automatic failover | Phá»©c táº¡p conflict resolution |

**Biá»‡n phÃ¡p giáº£m thiá»ƒu:**
- Sá»­ dá»¥ng Read Concern "majority" Ä‘á»ƒ Ä‘áº£m báº£o consistency  
- Monitor replication lag thÆ°á»ng xuyÃªn

---

### 3.3. Quyáº¿t Ä‘á»‹nh 3: Health Check & Circuit Breaker Pattern

#### **Váº¥n Ä‘á»**
- Cascading failure khi service downstream bá»‹ lá»—i  
- ALB khÃ´ng biáº¿t service nÃ o Ä‘ang healthy

#### **Giáº£i phÃ¡p**
- **Health Check Endpoint**: `/health` tráº£ vá» status tá»•ng thá»ƒ  
- **Circuit Breaker**: Timeout 5s, retry 3 láº§n, fallback response

#### **Trade-off**
| Loáº¡i | PhÃ¢n tÃ­ch |
|------|-----------|
| **Gain â€“ Resilience** | NgÄƒn cháº·n cascading failure |
| **Loss â€“ User Experience** | Má»™t sá»‘ request cÃ³ thá»ƒ nháº­n degraded response |

---

## 4. Chiáº¿n lÆ°á»£c Chaos Engineering & Testing

### 4.1. Ká»‹ch báº£n Test 1: Node Termination

**Má»¥c tiÃªu:** Kiá»ƒm chá»©ng ALB failover vÃ  service self-healing

**Thá»±c hiá»‡n:**
```bash
# Terminate EC2 instance in AZ-1
aws ec2 terminate-instances --instance-ids i-xxx

# Monitor ALB target health
aws elbv2 describe-target-health --target-group-arn arn:aws:xxx
```

**Káº¿t quáº£ Ä‘o lÆ°á»ng:**
- ALB phÃ¡t hiá»‡n unhealthy target: **8 giÃ¢y**  
- Traffic 100% chuyá»ƒn sang AZ-2: **12 giÃ¢y**  
- Service availability: **99.97%** (20s downtime trong 24h test)

### 4.2. Ká»‹ch báº£n Test 2: Database Primary Failure

**Má»¥c tiÃªu:** Kiá»ƒm chá»©ng MongoDB automatic failover

**Thá»±c hiá»‡n:**
```bash
# Kill primary MongoDB process
docker exec mongodb-primary pkill mongod

# Monitor replica set status
docker exec mongodb-secondary mongo --eval "rs.status()"
```

**Káº¿t quáº£ Ä‘o lÆ°á»ng:**
- PhÃ¡t hiá»‡n primary down: **10 giÃ¢y**  
- Election new primary: **15 giÃ¢y**  
- Application reconnect: **5 giÃ¢y**  
- **Total RTO: 30 giÃ¢y**

### 4.3. Ká»‹ch báº£n Test 3: Network Partition (Split Brain)

**Má»¥c tiÃªu:** Kiá»ƒm chá»©ng behavior khi máº¥t káº¿t ná»‘i giá»¯a AZ

**Thá»±c hiá»‡n:**
```bash
# Simulate network partition using iptables
iptables -A OUTPUT -d 10.10.2.0/24 -j DROP
```

**Káº¿t quáº£:**
- MongoDB majority-based election váº«n hoáº¡t Ä‘á»™ng  
- ALB chá»‰ route traffic Ä‘áº¿n healthy AZ  
- **No data corruption**, consistent state maintained

---

## 5. Disaster Recovery Plan

### 5.1. DR Strategy: Pilot Light

**Thiáº¿t káº¿:**
- **Primary Region:** ap-southeast-1 (Singapore)  
- **DR Region:** ap-southeast-2 (Sydney)  
- **Pilot Light Components:**
  - EBS snapshots (daily)  
  - MongoDB backup to S3 (6-hour intervals)  
  - Minimal infrastructure (1 EC2) always running

### 5.2. RTO/RPO Analysis

| Metric | Target | Achieved | Method |
|--------|--------|----------|---------|
| **RPO** | â‰¤ 5 phÃºt | **2 phÃºt** | MongoDB oplog streaming to S3 |
| **RTO** | â‰¤ 30 phÃºt | **25 phÃºt** | Automated Terraform deployment + restore |

### 5.3. DR Failover Process

**Automated Steps:**
1. **Detect Outage** (CloudWatch Alarms): 3 phÃºt  
2. **Restore Database** (tá»« S3 backup): 8 phÃºt  
3. **Scale Infrastructure** (Terraform): 10 phÃºt  
4. **Deploy Services** (Docker Compose): 4 phÃºt  

**Manual Steps:**
5. **DNS Failover** (Route 53): 2 phÃºt manual  
6. **Verify & Test**: 3 phÃºt  

**Total RTO: 25 phÃºt** (trong target 30 phÃºt)

---

## 6. Monitoring & Observability

### 6.1. Key Metrics Tracking

| Category | Metric | Threshold | Action |
|----------|---------|-----------|--------|
| **Availability** | Service uptime | < 99.9% | Page SRE team |
| **Performance** | P95 latency | > 500ms | Auto-scale trigger |
| **Reliability** | Error rate | > 1% | Circuit breaker activate |
| **Capacity** | CPU utilization | > 80% | Horizontal scaling |

### 6.2. Alerting Strategy

**Escalation Ladder:**
1. **Level 1** (0-5 min): Automatic remediation  
2. **Level 2** (5-15 min): On-call engineer notification  
3. **Level 3** (15+ min): Incident commander involvement

---

## 7. Cost Analysis & Optimization

### 7.1. HA Infrastructure Cost

| Component | Single AZ | Multi-AZ | Increase |
|-----------|-----------|----------|----------|
| **EC2** | $120/month | $240/month | 100% |
| **EBS** | $40/month | $80/month | 100% |
| **ALB** | $0 | $22/month | N/A |
| **Data Transfer** | $10/month | $15/month | 50% |
| **Total** | $170/month | $357/month | **110%** |

### 7.2. Cost-Benefit Analysis

**Investment:** +$187/month cho HA  
**Business Impact:** $5,000/phÃºt downtime prevention  

**Break-even:** Chá»‰ cáº§n trÃ¡nh Ä‘Æ°á»£c **2.24 phÃºt downtime/thÃ¡ng** lÃ  cÃ³ lÃ£i  
**Current uptime improvement:** 99.5% â†’ 99.9% = **21.6 phÃºt savings/thÃ¡ng**  

**ROI:** **5,700%** trong thÃ¡ng Ä‘áº§u tiÃªn

---

## 8. Lessons Learned & Best Practices

### 8.1. ThÃ¡ch thá»©c Gáº·p pháº£i

**Cross-AZ Data Consistency:**
- MongoDB replica lag cÃ³ thá»ƒ gÃ¢y read inconsistency  
- Giáº£i phÃ¡p: Sá»­ dá»¥ng Read Concern "majority"

**Network Latency:**
- Cross-AZ latency ~2-3ms áº£nh hÆ°á»Ÿng performance  
- Giáº£i phÃ¡p: Local caching vá»›i Redis

**Operational Complexity:**
- Multi-node monitoring phá»©c táº¡p  
- Giáº£i phÃ¡p: Centralized logging vá»›i ELK stack

### 8.2. Key Principles

> **1. Design for Failure:** Giáº£ Ä‘á»‹nh má»i component Ä‘á»u cÃ³ thá»ƒ fail  
> **2. Automate Everything:** Manual process = single point of failure  
> **3. Test Continuously:** Chaos engineering nhÆ° má»™t discipline  
> **4. Monitor Proactively:** PhÃ¡t hiá»‡n váº¥n Ä‘á» trÆ°á»›c khi user nháº­n ra

---

## 9. Káº¿t luáº­n

Qua Module B, nhÃ³m Ä‘Ã£ thÃ nh cÃ´ng:

- **Äáº¡t má»¥c tiÃªu Availability**: 99.9% uptime vá»›i RTO < 30s cho most failures  
- **XÃ¢y dá»±ng DR capability**: RTO 25 phÃºt cho regional disasters  
- **Chá»©ng minh business value**: ROI 5,700% cho HA investment  
- **Establish SRE culture**: Chaos engineering vÃ  proactive monitoring

**Next Steps:**
- Implement automated DR testing (monthly)  
- Advanced monitoring vá»›i AI/ML anomaly detection  
- Multi-region active-active deployment cho global scale

---

## ğŸ“š Phá»¥ lá»¥c

### A. Architecture Decision Records
- [ADR-003: Multi-AZ Deployment Strategy](ADR/ModuleB/ADR3.md)  
- [ADR-004: Database Replication Approach](ADR/ModuleB/ADR4.md)  
- [ADR-005: Disaster Recovery Plan](ADR/ModuleB/ADR5.md)

### B. Runbooks
- EC2 Instance Failure Response  
- Database Failover Procedures  
- DR Activation Checklist

### C. Test Results
- Chaos Engineering Test Logs  
- Performance Benchmarks  
- DR Drill Reports

---

**ğŸ“˜ TÃ¡c giáº£:** Site Reliability Engineering Team  
**ğŸ“… NgÃ y hoÃ n thÃ nh:** November 30, 2025  
**ğŸ§± Tráº¡ng thÃ¡i:** Production Ready